{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Spark SQL and Spark ML for simple data analytics\n",
    "--------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook show a few sample ways Spark SQL and Spark ML can be used to analyse data.\n",
    "We will be using the wine quality dataset from: https://archive.ics.uci.edu/ml/datasets/Wine+Quality, which captures various physical properties of wines (alcohol content, acidity etc) and their quality as apprised by wine experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating an sqlContext and loading the data from the csv file to a `df` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#load contents of winequality-white.csv to spark dataframe\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', delimiter=';')\\\n",
    "    .load('../data/winequality-white.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark (SQL) dataframes represent data of tabular structure (with rows and columns) and are somewhat similar to SQL tables, R or panads dataframes.\n",
    "\n",
    "Dataframes themselves do not actually store data - they are more recepies on how to apply series of transformations to datasets. To obtain actual data need to extract them from a dataframe into a python object in local memory.\n",
    "\n",
    "We can for example use `take` to retrieve first n rows of the data as a `list` of `Row` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(fixed acidity=7.0, volatile acidity=0.27, citric acid=0.36, residual sugar=20.7, chlorides=0.045, free sulfur dioxide=45.0, total sulfur dioxide=170.0, density=1.001, pH=3.0, sulphates=0.45, alcohol=8.8, quality=6)\n",
      "Row(fixed acidity=6.3, volatile acidity=0.3, citric acid=0.34, residual sugar=1.6, chlorides=0.049, free sulfur dioxide=14.0, total sulfur dioxide=132.0, density=0.994, pH=3.3, sulphates=0.49, alcohol=9.5, quality=6)\n",
      "Row(fixed acidity=8.1, volatile acidity=0.28, citric acid=0.4, residual sugar=6.9, chlorides=0.05, free sulfur dioxide=30.0, total sulfur dioxide=97.0, density=0.9951, pH=3.26, sulphates=0.44, alcohol=10.1, quality=6)\n"
     ]
    }
   ],
   "source": [
    "for r in df.take(3):\n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can  also easily convert a spark dataframe to pandas to display it. \n",
    "\n",
    "Be aware however that spark dataframes can contain billions of rows, and it may not be practical to convert them in their entirety (you will run out of RAM). We can subset the spark datafame first using for example `limit` to take only n first elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take 3 first rows and convert is to pandas\n",
    "df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every dataframe has an associated schema, which describes its colums, their types and other properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fixed acidity: double (nullable = true)\n",
      " |-- volatile acidity: double (nullable = true)\n",
      " |-- citric acid: double (nullable = true)\n",
      " |-- residual sugar: double (nullable = true)\n",
      " |-- chlorides: double (nullable = true)\n",
      " |-- free sulfur dioxide: double (nullable = true)\n",
      " |-- total sulfur dioxide: double (nullable = true)\n",
      " |-- density: double (nullable = true)\n",
      " |-- pH: double (nullable = true)\n",
      " |-- sulphates: double (nullable = true)\n",
      " |-- alcohol: double (nullable = true)\n",
      " |-- quality: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case the schema was inferred from the data in the 'winequality-white.csv' file, and spark is usually able to infer or retrieve the schema from variety of sources (e.g. JSON or parquet files). But it is also possible to define the schema expclicitely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how may rows a dataframe has with `count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4898"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataframe is very small. In fact it is so small that we could easily analyse using standard python tools. However the code below,  which is currenlty running on a sigle computer can be easily run on large spark clusters to analyse dataframes with billions of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing data with (Spark) SQL\n",
    "\n",
    "With Spark SQL we can treat dataframes as relational tabeles and query them using SQL like language (stricly speaking its HQL - Hive Query Language)\n",
    "\n",
    "For example we can register our dataframe as temporary table named `wine` and then use SQL to calculate the average alcohol content per wine quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>avg_alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>10.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>10.152454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>9.808840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>10.575372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>11.367936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>11.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>12.180000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quality  avg_alcohol\n",
       "0        3    10.345000\n",
       "1        4    10.152454\n",
       "2        5     9.808840\n",
       "3        6    10.575372\n",
       "4        7    11.367936\n",
       "5        8    11.636000\n",
       "6        9    12.180000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.registerTempTable('wine')\n",
    "sqlContext.sql(\"SELECT quality, avg(alcohol) AS avg_alcohol FROM wine GROUP BY quality\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning with Spark ML\n",
    "\n",
    "Here we will show a few example of using Spark ML to build regression models, predicting the quality of wine based it its properties.\n",
    "\n",
    "First we need to preprocess our data a bit. Spark ML works primarily on `double` values and our `quality` is currently of type `int`. We can use the following SQL statement to convert it to `double` in column `label`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[fixed acidity: double, volatile acidity: double, citric acid: double, residual sugar: double, chlorides: double, free sulfur dioxide: double, total sulfur dioxide: double, density: double, pH: double, sulphates: double, alcohol: double, quality: int, label: double]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sqlContext.sql(\"SELECT *, CAST(quality AS DOUBLE) AS label FROM wine\")\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have called the new dataframe `data` and applied caching to it. This will tell Spark to try to cache the data in memory for faster access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to build a simple lineary regression model that predicts wine quality `label` based on its properties. Spark requires that all the predictors (features) are combined into a single feature vector. We can use VectorAssemble to build it from selected columns of our dataframe (we will use all properties):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
      "Row(features=DenseVector([7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.001, 3.0, 0.45, 8.8]), label=6.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "input_colums=map(lambda s:s.name,df.schema.fields)[:-1]\n",
    "print \"Selected columns: %s\" % input_colums\n",
    "assembler = VectorAssembler(inputCols=input_colums, outputCol=\"features\")\n",
    "output = assembler.transform(data)\n",
    "print(output.select(\"features\", \"label\").first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the assembler to `transform` our input dataframe into one that includes the feature vector (`features`) and the response (`label`) as show above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to build a simple regession model. We will first split our data into traning and testing set and then train a regression model on the former one. Rather then using our transformed `output` we will chaing the preprocessing (vector assemblling) and model traning into a simple `pipeline` that will alow us to use our original data. In general pipelines may include may steps dealing with feature preprocessing, extraction etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "lr = LinearRegression(maxIter=30, regParam=0.3, elasticNetParam=0.25, featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now retrieve the linear regression model from the pipeline (stage 1) and look at the coefficients:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fixed acidity', -0.00013430053062177215)\n",
      "('volatile acidity', -0.8691000065905593)\n",
      "('citric acid', 0.0)\n",
      "('residual sugar', 0.0)\n",
      "('chlorides', -0.83022455907481618)\n",
      "('free sulfur dioxide', 0.0)\n",
      "('total sulfur dioxide', 0.0)\n",
      "('density', 0.0)\n",
      "('pH', 0.0)\n",
      "('sulphates', 0.0)\n",
      "('alcohol', 0.2022190460679141)\n"
     ]
    }
   ],
   "source": [
    "lrModel = model.stages[1]\n",
    "# summary only\n",
    "for t in zip(input_colums, lrModel.coefficients):\n",
    "    print t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that for most properties the coefficients are zero, which means that they do not contribute (accoriding to this specific model) to wine quality. This model is using elastic net regularisation which naturally 'selects' the most importnat variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the testing set to evaluate the peformance of our model with Root Mean Squared Error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.796034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark-1.6.1/python/pyspark/ml/regression.py:123: UserWarning: weights is deprecated. Use coefficients instead.\n",
      "  warnings.warn(\"weights is deprecated. Use coefficients instead.\")\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some insights into how good the model is we can try to compare it to the 'zero' model that predicts the mean of `quality`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of 'zero model' on test data = 0.891492\n",
      "R2 = 0.202688\n"
     ]
    }
   ],
   "source": [
    "mean_quality = trainingData.groupBy().avg('label').first()[0]\n",
    "zero_prediction = testData.selectExpr('label',\"CAST(%s AS DOUBLE) AS prediction\" % mean_quality)\n",
    "zero_rmse = evaluator.evaluate(zero_prediction)\n",
    "print(\"RMSE of 'zero model' on test data = %g\" % zero_rmse)\n",
    "R2= (zero_rmse**2-rmse**2)/zero_rmse**2\n",
    "print \"R2 = %g\"% R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that our model explains about 20% of variablity in data, which is rather a weak result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we can get better results if we can tune the parameters of the model. \n",
    "\n",
    "Spark ML comes with a ready to use parameter optimiser that uses cross validation from select the set of parameter from given search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "search_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.0, 0.3, 0.6]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.4, 0.6, 0.8]).build()\n",
    "    \n",
    "cv = CrossValidator(estimator = pipeline, estimatorParamMaps = search_grid, evaluator = evaluator, numFolds = 3)\n",
    "cv_model = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_predictions = cv_model.transform(testData)\n",
    "cv_rmse = evaluator.evaluate(cv_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % cv_rmse)\n",
    "cv_R2= (zero_rmse**2-cv_rmse**2)/zero_rmse**2\n",
    "print \"R2 = %g\"% cv_R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is slightly better explaining 27% of variance but still not very good.\n",
    "\n",
    "We can now use matplot lib to for example visualise residuals vs predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_values = np.array([r.label for r in cv_predictions.select('label').collect()])\n",
    "predicted_values = np.array([r.prediction for r in cv_predictions.select('prediction').collect()])\n",
    "residuals = predicted_values - true_values\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(predicted_values, residuals, c=true_values, cmap=plt.get_cmap('RdYlGn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that overall this model tends to overstimate quality of bad wines and underestimate quality of good ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But perhaps we can do better with a more complex model? Let's try to use RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", numTrees=100, maxBins=128, maxDepth=20, \\\n",
    "                           minInstancesPerNode=5, seed=33)\n",
    "rf_pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "rf_model = rf_pipeline.fit(trainingData)\n",
    "# Make predictions.\n",
    "rf_predictions = rf_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_rmse = evaluator.evaluate(rf_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rf_rmse)\n",
    "rf_R2= (zero_rmse**2-rf_rmse**2)/zero_rmse**2\n",
    "print \"R2 = %g\"% rf_R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest gives a much better model that explains 42% of variance. This is an improvement over the linear models but overall the perfomance is rather week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualisation\n",
    "\n",
    "Here we will use Principal Component Analysis to reduce the data dimesionalit so that we can plot the in 2D space.\n",
    "\n",
    "As before we will create a multi step pipeline that will:\n",
    "\n",
    "* assemble the feature vector from all predictors\n",
    "* normalise the features (to 0 mean and 1 stddev) \n",
    "* extract two most significant PCA components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "input_cols = map(lambda s:s.name,df.schema.fields)[0:-1]\n",
    "print \"Input features: %s\" % input_cols\n",
    "\n",
    "\n",
    "all_assembler = VectorAssembler(\n",
    "    inputCols=input_cols,\n",
    "    outputCol=\"features\")\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"norm_features\")\n",
    "pca = PCA(k=2, inputCol=\"norm_features\", outputCol=\"pca_features\")\n",
    "\n",
    "pca_pipeline = Pipeline(stages=[all_assembler, normalizer, pca])\n",
    "\n",
    "pca_model = pca_pipeline.fit(data)\n",
    "\n",
    "pca_model.transform(data).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The `pca_features` column now contains the reduced 2D representation of all other features.\n",
    "\n",
    "We can now use it to visualise a 30% sample of the data. The color represents quality of wine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pca_data = pca_model.transform(data)\n",
    "\n",
    "sampling_fraction = 0.3\n",
    "\n",
    "pca_xy = np.matrix(map(lambda r:r.pca_features.array, pca_data.sample(False, sampling_fraction, 13).collect()))\n",
    "pca_colors = map(lambda r: float(r.quality),data.select('quality').sample(False, sampling_fraction, 13).collect())\n",
    "\n",
    "plt.scatter(pca_xy[:,0], pca_xy[:,1], c=pca_colors, alpha=0.4, cmap=plt.get_cmap('RdYlGn'), edgecolors='none', s=50)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Unfortunatelly Spark ML currently does not provide information on how much variance is explained by the two component or what they are.\n",
    "Still there are some obvious areas dominated by good and bad quality wines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
